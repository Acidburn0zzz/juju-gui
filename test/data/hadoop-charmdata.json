{
  "maintainer": "James Page <james.page@ubuntu.com>",
  "owner": "charmers",
  "series": "precise",
  "provides": {
    "namenode": {
      "interface": "dfs"
    },
    "ganglia": {
      "interface": "monitor"
    },
    "jobtracker": {
      "interface": "mapred"
    }
  },
  "config": {
    "options": {
      "tasktracker.http.threads": {
        "default": 40,
        "type": "int",
        "description": "The number of worker threads that for the http server. This is used for\nmap output fetching.\n"
      },
      "webhdfs": {
        "default": false,
        "type": "boolean",
        "description": "Hadoop includes a RESTful API over HTTP to HDFS.  Setting this flag\nto True enables this part of the HDFS service.\n"
      },
      "mapred.child.java.opts": {
        "default": "-Xmx200m",
        "type": "string",
        "description": "Java opts for the task tracker child processes. The following symbol,\nif present, will be interpolated: @taskid@ is replaced by current TaskID.\nAny other occurrences of '@' will go unchanged. For example, to enable\nverbose gc logging to a file named for the taskid in /tmp and to set\nthe heap maximum to be a gigabyte, pass a 'value' of:\n.\n  -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc\n.\nThe configuration variable mapred.child.ulimit can be used to control\nthe maximum virtual memory of the child processes.\n"
      },
      "heap": {
        "default": 1024,
        "type": "int",
        "description": "The maximum heap size in MB to allocate for daemons processes within the\nservice units managed by this charm.\n.\nThe recommended configurations vary based on role and the amount of\nraw disk storage available in the hadoop cluster:\n.\n * NameNode: 1GB of heap for every 100TB of raw data stored.\n * SecondaryNameNode: Must be paired with the NameNode.\n * JobTracker: 2GB.\n * DataNode: 1GB.\n * TaskTracker: 1GB.\n.\nThe above recommendations are taken from HBase: The Definitive Guide by\nLars George.\n.\nObviously you need to ensure that the servers supporting each service unit\nhave sufficient memory to accomodate this setting - it should be no more\nthan 75% of the total memory in the system excluding swap.\n.\nIf you are also mixing MapReduce and DFS roles on the same units you need to\ntake this into account as well (see README for more details).\n"
      },
      "io.sort.factor": {
        "default": 10,
        "type": "int",
        "description": "The number of streams to merge at once while sorting files. This\ndetermines the number of open file handles.\n"
      },
      "io.file.buffer.size": {
        "default": 4096,
        "type": "int",
        "description": "The size of buffer for use in sequence files. The size of this buffer should\nprobably be a multiple of hardware page size (4096 on Intel x86), and it\ndetermines how much data is buffered during read and write operations.\n"
      },
      "io.sort.mb": {
        "default": 100,
        "type": "int",
        "description": "The total amount of buffer memory to use while sorting files, in\nmegabytes. By default, gives each merge stream 1MB, which should minimize\nseeks.\n"
      },
      "pig": {
        "default": false,
        "type": "boolean",
        "description": "To install Apache Pig on all service units alongside Hadoop set this\nconfiguration to 'True'.\n.\nApache Pig is a platform for analyzing large data sets that consists\nof a high-level language for expressing data analysis programs, coupled\nwith infrastructure for evaluating these programs. The salient property\nof Pig programs is that their structure is amenable to substantial\nparallelization, which in turns enables them to handle very large data\nsets.\n"
      },
      "mapred.reduce.parallel.copies": {
        "default": 5,
        "type": "int",
        "description": "The default number of parallel transfers run by reduce during the\ncopy(shuffle) phase.\n"
      },
      "source": {
        "default": "stable",
        "type": "string",
        "description": "Location and packages to install hadoop:\n.\n * dev:     Install using the hadoop packages from\n            ppa:hadoop-ubuntu/dev.\n * testing: Install using the hadoop packages from\n            ppa:hadoop-ubuntu/testing.\n * stable:  Install using the hadoop packages from\n            ppa:hadoop-ubuntu/stable.\n.\nThe packages provided in the hadoop-ubuntu team PPA's are based\ndirectly on upstream hadoop releases but are not fully built from\nsource.\n"
      },
      "mapred.job.tracker.handler.count": {
        "default": 10,
        "type": "int",
        "description": "The number of server threads for the JobTracker. This should be roughly\n4% of the number of tasktracker nodes.\n"
      },
      "hadoop.dir.base": {
        "default": "/var/lib/hadoop",
        "type": "string",
        "description": "The directory under which all other hadoop data is stored.  Use this\nto take advantage of extra storage that might be avaliable.\n.\nYou can change this in a running deployment but all existing data in\nHDFS will be inaccessible; you can of course switch it back if you\ndo this by mistake.\n"
      },
      "dfs.namenode.handler.count": {
        "default": 10,
        "type": "int",
        "description": "The number of server threads for the namenode.  Increase this in larger\ndeployments to ensure the namenode can cope with the number of datanodes\nthat it has to deal with.\n"
      },
      "hbase": {
        "default": false,
        "type": "boolean",
        "description": "Setting this configuration parameter to 'True' configures HDFS for\nuse with HBase including turning on 'append' mode which is not\ndesirable in all deployment scenarios.\n"
      },
      "dfs.block.size": {
        "default": 67108864,
        "type": "int",
        "description": "The default block size for new files (default to 64MB).  Increase this in \nlarger deployments for better large data set performance.\n"
      },
      "dfs.datanode.max.xcievers": {
        "default": 4096,
        "type": "int",
        "description": "The number of files that an datanode will serve at any one time.\n.\nAn Hadoop HDFS datanode has an upper bound on the number of files that it\nwill serve at any one time. This defaults to 256 (which is low) in hadoop\n1.x - however this charm increases that to 4096.\n"
      }
    }
  },
  "description": "Hadoop is a software platform that lets one easily write and\nrun applications that process vast amounts of data.\n.\nHere's what makes Hadoop especially useful:\n.\n* Scalable: Hadoop can reliably store and process petabytes.\n* Economical: It distributes the data and processing across clusters\n  of commonly available computers. These clusters can number\n  into the thousands of nodes.\n* Efficient: By distributing the data, Hadoop can process it in parallel\n  on the nodes where the data is located. This makes it\n  extremely rapid.\n* Reliable: Hadoop automatically maintains multiple copies of data and\n  automatically redeploys computing tasks based on failures.\n.\nHadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).\nMapReduce divides applications into many small blocks of work. HDFS creates\nmultiple replicas of data blocks for reliability, placing them on compute\nnodes around the cluster. MapReduce can then process the data where it is\nlocated.\n",
  "store_url": "cs:precise/hadoop-5",
  "requires": {
    "secondarynamenode": {
      "interface": "dfs"
    },
    "datanode": {
      "interface": "dfs"
    },
    "tasktracker": {
      "interface": "mapred"
    },
    "mapred-namenode": {
      "interface": "dfs"
    }
  },
  "store_revision": 5,
  "name": "hadoop",
  "summary": "Software platform for processing vast amounts of data",
  "bzr_branch": "lp:~charmers/charms/precise/hadoop/trunk",
  "last_change": {
    "committer": "Marc Cluet <marc.cluet@ubuntu.com>",
    "message": "* Merged james-page storage option into charm\n",
    "revno": 27,
    "created": 1339074711.627
  },
  "proof": {}
}
